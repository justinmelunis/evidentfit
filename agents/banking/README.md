# Agent C: Banking Initialization

## Overview

The Banking Initialization Agent pre-computes and caches all evidence grades and reasoning for the 3-level banking system:

- **Level 1**: Goal × Supplement Evidence Grades (162 combinations - 6 goals × 27 supplements)
- **Level 2**: Profile-Specific Reasoning (7,290 combinations - 270 profiles × 27 supplements)

**Note**: We search for 63 supplements in our research database, but bank evidence for the top 27 most important supplements based on research volume and user demand. 
- **Level 3**: Real-time adjustments (never cached)

## Purpose

This agent runs when:
- Major research updates occur (new papers ingested)
- New profile combinations are needed
- Banking cache needs refresh

## Architecture

### Three-Level Banking System

**Level 1: Goal × Supplement Evidence (162 combinations)**
- Pre-computed evidence grades for supplement × goal combinations (6 goals × 27 supplements)
- **Note**: We search for 63 supplements in ingestion, but bank the top 27 most important supplements
- **LLM Research Agent**: Reads and analyzes research papers to assign evidence grades (A/B/C/D)
- Grades based on research outcomes (positive/negative/mixed results), not just paper count
- **Supporting Publications**: Each evidence grade includes top 3 supporting research papers with full metadata
- Updated monthly when new research is ingested

**Level 2: Profile-Specific Reasoning (7,290 combinations)**
- Personalized "why" explanations based on demographics
- Generated by LLM analysis of research papers tailored to user profile
- **Profile-Specific Publications**: Each reasoning includes supporting research papers relevant to the user's demographic profile
- Profile combinations: 6 goals × 5 weight bins × 3 sexes × 3 age bins = 270 profiles
- Age bins: young (18-29), adult (30-49), mature (50+) - pediatric populations excluded
- Total combinations: 270 profiles × 27 supplements = 7,290 reasoning entries

**Level 3: Real-Time Context Analysis (Never cached)**
- Dynamic modifications based on conversation context
- Text parsing for conditions, medications, specific interests
- **User-Specific Publications**: Real-time research papers relevant to user's specific context and conditions
- Applied in real-time for maximum personalization

## Files

- `run.py` - Main banking initialization script
- `run_banking_init.py` - Entry point script with environment setup
- `banking_loader.py` - Banking cache loader for API
- `level1_evidence_bank.json` - Level 1 banking cache
- `level2_reasoning_bank.json` - Level 2 banking cache
- `banking_summary.json` - Banking metadata and summary
- `banking_init.env` - Environment variables template

## Usage

### Local Development
```bash
cd agents/banking
python run_banking_init.py
```

### Docker
```bash
docker build -t evidentfit-banking .
docker run --env-file banking_init.env evidentfit-banking
```

## Environment Variables

Required environment variables (see `banking_init.env`):
- `FOUNDATION_ENDPOINT` - Azure AI Foundry endpoint
- `FOUNDATION_KEY` - Azure AI Foundry API key
- `SEARCH_ENDPOINT` - Azure AI Search endpoint
- `SEARCH_QUERY_KEY` - Azure AI Search query key
- `INDEX_VERSION` - Current index version

## Output

The agent generates:
- `level1_evidence_bank.json` - 162 evidence grades with supporting publications
- `level2_reasoning_bank.json` - 360 profile-specific reasoning sets
- `banking_summary.json` - Metadata and summary information
- `banking_init.log` - Detailed logging of the initialization process

## Integration

The banking caches are consumed by:
- **Agent F (Stack Builder)**: Loads banking data for fast personalized recommendations
- **Agent E (Research Chat)**: Uses banking for evidence-based responses
- **Frontend**: Exposes Level 1 data via `/supplements/evidence` endpoint
- **Stack Planner**: Uses all three levels for comprehensive personalization

## Model Selection

**Model**: GPT-4o-mini (Azure AI Foundry)

**Why GPT-4o-mini?**
- ✅ **Cost-effective**: ~$10.50 per run (~$42/year for quarterly updates with 63 supplements)
- ✅ **Fast**: 20-30 minutes with parallel execution (vs 8-12 hours local)
- ✅ **High quality**: Excellent multi-document synthesis and citation accuracy
- ✅ **Consistent**: Same model used for user-facing API responses
- ✅ **Reliable**: 99.9% uptime, fully managed by Azure
- ✅ **Proven**: Production-ready and validated for health recommendations

**Decision**: GPT-4o-mini is our long-term choice for banking. We've evaluated alternatives and determined it provides the best balance of cost, quality, and speed for evidence grading tasks.

**Alternatives evaluated:**
- **GPT-4o**: 17× more expensive (~$700/year) for minimal quality gain (~5-10% better)
- **Mistral-7B/Llama-3.1-8B (local)**: Saves ~$42/year but 60× slower, quality validation needed, higher citation risk
- **Other cloud models**: Not in Azure AI Foundry, would add operational complexity

See [Model Selection Strategy](../../docs/MODEL_SELECTION.md) for detailed cost analysis.

## Performance (63 supplements)

- **Level 1**: 378 LLM calls (6.5k input + 600 output tokens each) = 2.46M input + 0.23M output (6 goals × 63 supplements)
- **Level 2**: 17,010 LLM calls (2.5k input + 250 output tokens each) = 42.5M input + 4.25M output (270 profiles × 63 supplements)
- **Total**: ~45M input tokens + ~4.5M output tokens per run
- **Runtime**: 20-30 minutes (parallel execution with 100+ concurrent calls)
- **Cost**: ~$10.50 per run (~$42/year for quarterly updates)
- **Storage**: ~6MB total for all banking caches

## Monitoring

The agent provides detailed logging:
- Progress updates every 20 Level 1 combinations
- Progress updates every 50 Level 2 combinations
- Individual supplement grades and paper counts
- Error handling and retry logic
- Final summary with completion statistics
