# Agent E: Banking Initialization

## Overview

The Banking Initialization Agent pre-computes and caches all evidence grades and reasoning for the 3-level banking system:

- **Level 1**: Goal × Supplement Evidence Grades (162 combinations)
- **Level 2**: Profile-Specific Reasoning (360 combinations) 
- **Level 3**: Real-time adjustments (never cached)

## Purpose

This agent runs when:
- Major research updates occur (new papers ingested)
- New profile combinations are needed
- Banking cache needs refresh

## Architecture

### Three-Level Banking System

**Level 1: Goal × Supplement Evidence (162 combinations)**
- Pre-computed evidence grades for every supplement × goal combination (6 goals × 27 supplements)
- **LLM Research Agent**: Reads and analyzes research papers to assign evidence grades (A/B/C/D)
- Grades based on research outcomes (positive/negative/mixed results), not just paper count
- **Supporting Publications**: Each evidence grade includes top 3 supporting research papers with full metadata
- Updated monthly when new research is ingested

**Level 2: Profile-Specific Reasoning (360 combinations)**
- Personalized "why" explanations based on demographics
- Generated by LLM analysis of research papers tailored to user profile
- **Profile-Specific Publications**: Each reasoning includes supporting research papers relevant to the user's demographic profile
- Profile combinations: 6 goals × 5 weight bins × 3 sexes × 4 age bins = 360 combinations

**Level 3: Real-Time Context Analysis (Never cached)**
- Dynamic modifications based on conversation context
- Text parsing for conditions, medications, specific interests
- **User-Specific Publications**: Real-time research papers relevant to user's specific context and conditions
- Applied in real-time for maximum personalization

## Files

- `run.py` - Main banking initialization script
- `run_banking_init.py` - Entry point script with environment setup
- `banking_loader.py` - Banking cache loader for API
- `level1_evidence_bank.json` - Level 1 banking cache
- `level2_reasoning_bank.json` - Level 2 banking cache
- `banking_summary.json` - Banking metadata and summary
- `banking_init.env` - Environment variables template

## Usage

### Local Development
```bash
cd agents/banking
python run_banking_init.py
```

### Docker
```bash
docker build -t evidentfit-banking .
docker run --env-file banking_init.env evidentfit-banking
```

## Environment Variables

Required environment variables (see `banking_init.env`):
- `FOUNDATION_ENDPOINT` - Azure AI Foundry endpoint
- `FOUNDATION_KEY` - Azure AI Foundry API key
- `SEARCH_ENDPOINT` - Azure AI Search endpoint
- `SEARCH_QUERY_KEY` - Azure AI Search query key
- `INDEX_VERSION` - Current index version

## Output

The agent generates:
- `level1_evidence_bank.json` - 162 evidence grades with supporting publications
- `level2_reasoning_bank.json` - 360 profile-specific reasoning sets
- `banking_summary.json` - Metadata and summary information
- `banking_init.log` - Detailed logging of the initialization process

## Integration

The banking caches are consumed by:
- **Agent B (User API)**: Loads banking data for fast personalized recommendations
- **Frontend**: Exposes Level 1 data via `/supplements/evidence` endpoint
- **Stack Planner**: Uses all three levels for comprehensive personalization

## Model Selection

**Model**: GPT-4o-mini (Azure AI Foundry)

**Why GPT-4o-mini?**
- ✅ **Cost-effective**: $5.32 per run (~$21/year for quarterly updates)
- ✅ **Fast**: 20-30 minutes with parallel execution (vs 8-12 hours local)
- ✅ **High quality**: Excellent multi-document synthesis and citation accuracy
- ✅ **Consistent**: Same model used for user-facing API responses
- ✅ **Reliable**: 99.9% uptime, fully managed by Azure

**Alternatives considered:**
- **GPT-4o**: 17× more expensive ($88/run) for minimal quality gain
- **Llama-3.1-8B (local)**: Saves $19/year but 60× slower, quality validation needed

See [Model Selection Strategy](../../docs/MODEL_SELECTION.md) for detailed cost analysis.

## Performance

- **Level 1**: 162 LLM calls (6.5k input + 600 output tokens each)
- **Level 2**: 9,720 LLM calls (2.5k input + 250 output tokens each)
- **Total**: ~25.35M input tokens + ~2.53M output tokens per run
- **Runtime**: 20-30 minutes (parallel execution with 100+ concurrent calls)
- **Cost**: $5.32 per run ($21.28/year for quarterly updates)
- **Storage**: ~3MB total for all banking caches

## Monitoring

The agent provides detailed logging:
- Progress updates every 20 Level 1 combinations
- Progress updates every 50 Level 2 combinations
- Individual supplement grades and paper counts
- Error handling and retry logic
- Final summary with completion statistics
