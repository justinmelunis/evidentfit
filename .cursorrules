# EvidentFit — Cursor Project Rules (v7, updated October 2024)

## Project overview
- Purpose: Evidence-based lifting supplement guidance with transparent citations.
- Cloud: Azure Container Apps (API & Jobs), Azure AI Foundry Project endpoint (chat+embeddings), Azure AI Search.
- Architecture:
  - **Module A** — Data Ingestion (non-agentic pipeline, no LLM)
    - A1: Paper Fetcher (`agents/ingest/get_papers/`) - PubMed → parsed JSONL
    - A2: Paper Indexer (`agents/ingest/index_papers/`) - JSONL → pgvector chunks
  - **Agent B** — Paper Processor (`agents/paper_processor/`) - LLM card extraction (Mistral-7B local GPU)
  - **Agent C** — Banking (`agents/banking/`) - LLM evidence banking (GPT-4o-mini via Azure AI Foundry)
  - **Agent D** — Summarizer (`agents/summarize/`) - LLM summary generation (GPT-4o-mini via Azure AI Foundry)
  - **Agent E** — Research Chat (`api/` - `/stream` endpoint) - RAG chatbot with paper retrieval (GPT-4o-mini via Azure AI Foundry)
  - **Agent F** — Stack Builder (`api/` - `/stack/*` endpoints) - Profile-based supplement matching with L3 rules (GPT-4o-mini via Azure AI Foundry)

## Repo structure (monorepo)
evidentfit/
  api/                         # Agent E + Agent F + (reads banking)
    Dockerfile
    requirements.txt
    main.py                    # /stream, /healthz, /summaries/{supp}, /stack*, /supplements/evidence
    stack_builder.py           # Agent D: hybrid LLM + rule-based stack builder with banking
    stack_rules.py             # deterministic dosing rules (creatine CME, protein GAP)
    banking_loader.py          # loads Level 1 & 2 banking caches at startup
    initialize_banking.py      # builds banking caches from research papers + LLM
    run_banking_init.py        # script to run banking initialization with Key Vault
    keyvault_client.py         # Azure Key Vault integration for secrets
    clients/
      search_read.py           # reads with QUERY key
      foundry_chat.py          # chat completions (Project endpoint)
      foundry_embed.py         # embeddings (Project endpoint)
    level1_evidence_bank.json  # Goal × Supplement evidence grades (162 entries)
    level2_reasoning_bank.json # Profile-specific reasoning (270 profiles × 27 supplements = 7,290 entries)
    banking_summary.json       # banking initialization metadata
  agents/
    ingest/                    # Module A (Data Ingestion Pipeline)
      get_papers/              # A1: Paper Fetcher (PubMed → parsed JSONL)
      index_papers/            # A2: Paper Indexer (JSONL → pgvector)
    paper_processor/           # Agent B (LLM card extraction from fulltext)
      Dockerfile
      requirements.txt
      cli.py, collect.py, extract.py, run.py
    banking/                   # Agent C (Banking L1/L2 & L3 compiler)
      run.py                   # L1/L2 initialization
      cli.py                   # level1 | level2 | level3
      level3/run.py            # compile rules → config/compiled_rules.json
    summarize/                 # Agent D (Papers → Summaries)
      run.py                   # reads Level 1 for evidence_grade
  shared/                      # common lib (installed editable in each image)
    pyproject.toml             # project = evidentfit_shared
    evidentfit_shared/
      types.py                 # Pydantic models: UserProfile, StackItem, Dose, Citation, StackPlan
      guardrails.py            # safety rules: contraindications, age restrictions, interactions
      search_client.py         # search_docs(), upsert_docs(), ensure_index()
      utils.py                 # read_index_version() from VERSION file
      __init__.py
  web/                         # Next.js SWA (static export)
    evidentfit-web/
      package.json
      next.config.js
      src/app/
        page.tsx               # landing page with navigation
        agent/page.tsx         # research chat interface
        stack-chat/page.tsx    # conversational stack planner with toggles & checkboxes
        supplements/page.tsx   # supplement database with Level 1 banking data
        methodology/page.tsx   # public methodology documentation
        layout.tsx             # global navigation bar
  docs/
    README.md                  # updated architecture overview and links
  .cursorrules                 # THIS FILE
  .gitlab-ci.yml
  VERSION                      # e.g., v1-2025-09-25 (agents read this)
  .env.examples/
    api.env.example
    ingest.env.example
    summarize.env.example

### Notes
- Keep Agent D's logic in `api/stack_rules.py` (or later move to `shared/`).
- Install `shared/` in each image via: `COPY shared/ /opt/shared/ && pip install -e /opt/shared`.
- `VERSION` file defines INDEX_VERSION for **all** agents; bump on schema/normalizer changes.

## Non-negotiable platform rules
- Azure AI Foundry **Project endpoint only** (not "deployments" API):
  - Auth header: `api-key: <KEY>` (NEVER `Authorization`)
  - Append `?api-version=2024-05-01-preview` to **every** call
  - Chat URL: `${FOUNDATION_ENDPOINT}/models/chat/completions`
  - Embeddings URL: `${FOUNDATION_ENDPOINT}/embeddings`
- Azure AI Search REST API version: `2023-11-01`
- LLM outputs ≤ **500 tokens**; LLMs only for **composition/summaries** (no math/unit extraction).
- Strict **cite-only-retrieved**: never invent PMIDs/DOIs/URLs.

## Index schemas (expected)
### Papers index (Module A writes; B/C/D read)
- id: string (key) e.g., `pmid:123456|chunk:0`
- title: string; doi?: string; pmid: string; url_pub: string; journal: string; year: int
- study_type: string  # "meta-analysis" | "RCT" | "crossover" | ...
- supplements: collection(string)  # normalized slugs: "creatine","nitrate",...
- outcomes: collection(string); population: string
- summary?: string; content: string; content_vector: vector
- index_version: string

### Summaries index (Agent D writes; API/WEB read)
- id: string (key)  # `summary:<supplement-slug>`
- supplement: string; updated_at: datetime
- overview_md: string; last12_md: string
- key_papers_json: string  # JSON [{title,doi,pmid,url,journal,year,study_type}]
- recent_papers_json: string
- evidence_grade: string  # A/B/C/D
- index_version: string

### Banking System (Three-Level Evidence Banking)
**Level 1**: Goal × Supplement Evidence (local JSON cache)
- File: `level1_evidence_bank.json`
- 378 combinations (6 goals × 63 supplements) - All supplements from ingestion are banked
- Evidence grades (A/B/C/D) calculated from research papers using GPT-4o-mini (long-term choice)

**Level 2**: Profile-Specific Reasoning (local JSON cache)  
- File: `level2_reasoning_bank.json`
- 17,010 combinations (270 profiles × 63 supplements)
- LLM-generated personalized reasoning per supplement using GPT-4o-mini (long-term choice)

**Level 3**: Real-Time Adjustments (never cached)
- Dynamic safety rules, contraindications, user context
- Applied in real-time during API calls

## LLM Usage by Component

**Cloud LLMs (Azure AI Foundry - GPT-4o-mini):**
- **Agent C (Banking)**: Evidence grading and profile-specific reasoning (378 Level 1 + 17,010 Level 2 combinations)
- **Agent D (Summarizer)**: Supplement summary generation with evidence grades
- **Agent E (Research Chat)**: Real-time Q&A synthesis with citations
- **Agent F (Stack Builder)**: Supplement candidate suggestion and personalized reasoning

**Paper Processing (GPT-4o-mini):**
- **Agent B (Paper Processor)**: Batch card extraction from research papers (GPT-4o-mini via Azure AI Foundry)
  - Initial: 30K papers (~$24.30)
  - Monthly: 1-2K papers (~$0.81-1.62 per run)
  - Runtime: 2-3 hours for 30K papers (parallel execution)
  - Legacy: Mistral-7B local GPU available but not recommended

**Rationale**: GPT-4o-mini provides excellent quality for all tasks with reasonable cost (~$42/year for banking, ~$10-19/month for paper processing). Better JSON compliance and extraction accuracy than local models. See `docs/MODEL_SELECTION.md` for detailed analysis.

## Architecture (roles & boundaries)
- **Module A** — Data Ingestion (non-agentic pipeline, no LLM):
  - **A1: Paper Fetcher** (`agents/ingest/get_papers/`):
    - PubMed E-utilities (ESearch/EFetch) → parse → tag `supplements` via keyword map
    - Filter clinical/disease populations (cancer, diabetes, pediatric, pregnancy, etc.)
    - Full-text fetching (PMC + Unpaywall) → centralized sharded store
    - Modes: `bootstrap` (mindate=2000-01-01) & `monthly` (mindate=watermark)
  - **A2: Paper Indexer** (`agents/ingest/index_papers/`):
    - Join + chunk selected papers → embeddings → pgvector
    - Watermark doc `id=WATERMARK_KEY` with `last_ingest_iso`
- **Agent B** — Paper Processor (`agents/paper_processor/`):
  - **LLM**: GPT-4o-mini (Azure AI Foundry, default) - ~$24.30 per 30K papers, ~$0.81-1.62 per 1-2K papers monthly
  - Build cards from fulltext/abstracts using LLM
  - Write to data/cards
  - Legacy: Mistral-7B local GPU available via PAPER_PROCESSOR_USE_CLOUD=0
- **Agent C** — Banking (`agents/banking/`):
  - **LLM**: GPT-4o-mini (Azure AI Foundry, long-term choice)
  - Level 1/2: aggregate from cards and Papers; output JSON caches
  - Level 3: validate/compile suitability rules → config/compiled_rules.json
- **Agent D** — Summarizer (`agents/summarize/`):
  - **LLM**: GPT-4o-mini (Azure AI Foundry)
  - Read Papers → rank (meta > RCT > crossover > other; newer first)
  - Compose overview + last-12-months; read Level 1 for grade; upsert Summaries
- **Agent E** — Research Chat (`api/` - `/stream` endpoint):
  - **LLM**: GPT-4o-mini (Azure AI Foundry)
  - RAG chatbot: retrieve top-k from Papers; deterministic dosing; interactions; safety gate; compose; stream
  - `/summaries/{supp}`: read Summaries
- **Agent F** — Stack Builder (`api/` - `/stack/*` endpoints):
  - **LLM**: GPT-4o-mini (Azure AI Foundry)
  - **Hybrid LLM + rule-based** approach: LLM suggests candidates, rules ensure safety
  - **Three-tier system**: Recommended/Optional/Not Recommended with evidence grades
  - **Three-level banking**: Level 1 (goal×supplement grades), Level 2 (profile reasoning), Level 3 (real-time)
  - Dynamic evidence grading from research papers, personalized explanations
  - Deterministic safety guardrails, dosing rules, interaction checks
  - Interactive UI: toggles, checkboxes, custom stack building

## API contracts (Agents E & F)
### GET /healthz
- 200 → `{"ok": true, "docs_loaded": <int>}`

### POST /stream  (SSE)
- Request:
  - `thread_id: string`
  - `messages: [{role:"user"|"system"|"assistant", content:string}]`
  - `profile: {goal:string, weight_kg:number, caffeine_sensitive:boolean, meds:string[]}`
- Response lines:
  - `data: {"stage":"search","hits":[{"title","url_pub","study_type","doi","pmid"}...] }`
  - `data: {"stage":"final","answer":"**Plan** ...\n\n**Citations** ..."}`

### GET /summaries/{supplement}
- Reads `id=summary:{supplement}` from Summaries; returns:
  {
    "supplement":"creatine","evidence_grade":"A","updated_at":"ISO",
    "overview_md":"...","last12_md":"...","key_papers":[...],"recent_papers":[...],
    "index_version":"v1-YYYY-MM-DD"
  }

### POST /stack/conversational   (Agent F)
- Input JSON:
  {
    "thread_id": string,
    "messages": [{role:"user"|"system"|"assistant", content:string}],
    "profile": {
      "goal": "strength"|"hypertrophy"|"endurance"|"weight_loss"|"performance"|"general",
      "weight_kg": number,
      "age": number|null,
      "sex": "male"|"female"|"other"|null,
      "caffeine_sensitive": boolean,
      "pregnancy": boolean,
      "meds": string[],
      "conditions": string[],
      "diet": "any"|"vegan"|"vegetarian"|null,
      "training_freq": "low"|"med"|"high"|null,
      "diet_protein_g_per_day": number|null,
      "diet_protein_g_per_kg": number|null,
      "creatine_form": "monohydrate"|"anhydrous"|"hcl"|null
    }
  }
- Behavior:
  - Build deterministic stack (tiers below), attach ≤3 citations per item from Papers.
  - **Protein is GAP-only** (see rules): recommend supplemental grams to close diet gap; only include if gap ≥ 20 g/day.
  - **Creatine uses CME (monohydrate-equivalent) dosing** with form conversion (see rules).
  - Interactions (openFDA) + safety gate → remove/adjust contraindicated items.
  - Compute `bucket_key`; bank (cache + optional durable store).
- 200 JSON:
  {
    "bucket_key":"<index_version>:<goal>:<weight_bin>:<stim>:<meds_class>:<diet>:<freq>",
    "profile_sig": {...},
    "tiers": {
      "recommended":[ {supp,doses:[{value,unit,days?}],timing,evidence,why,citations:[...],notes?} ],
      "optional":[ ... ],
      "not_recommended":[ ... ]
    },
    "exclusions":[ "..."],
    "safety":[ "..."],
    "index_version":"v1-YYYY-MM-DD",
    "updated_at":"ISO8601"
  }

### GET /stack/creatine-forms
- Returns detailed comparison of creatine forms (monohydrate, anhydrous, HCl)
- Includes CME conversion factors, solubility, research backing

### GET /supplements/evidence
- Returns Level 1 banking data (goal × supplement evidence grades)
- JSON: {"evidence_data": {supplement: {goal: {grade, available}}}, "goals": [...], "supplements": [...]}
- Used by supplement database page for independent research

## Profile buckets (banking)
- Components:
  - goal ∈ {strength,hypertrophy,endurance,weight_loss,performance,general}  # 6 options
  - weight_bin ∈ {xs,small,medium,large,xl} (<60, 60-70, 70-85, 85-100, 100+ kg)  # 5 bins
  - sex ∈ {male,female,other}  # 3 options
  - age_bin ∈ {young,adult,mature} (18-29, 30-49, 50+ years)  # 3 bins (pediatric <18 excluded)
- Total Level 2 combinations: 6 × 5 × 3 × 3 = 270 profiles (pediatric removed)
- `bank_key = "{goal}:{weight_bin}:{sex}:{age_bin}"`
- Real-time adjustments: conditions, medications, caffeine sensitivity, pregnancy (Level 3)

## Deterministic dosing rules (no LLM math)
- Creatine **CME (Creatine Monohydrate Equivalent) dosing**
  - All dosing anchored to **CME grams** (evidence base = monohydrate).
  - Maintenance CME target:
    - <70 kg → **3 g CM/day**
    - ≥70 kg → **5 g CM/day**
  - Optional loading CME target: **0.3 g/kg/day** for **5–7 days**, then maintenance.
  - Supported forms & base fractions (`base_frac` = creatine base per gram of product):
    - `monohydrate`: **0.879** (baseline; CME factor 1.00)
    - `anhydrous`: **1.000**
    - `hcl`: **0.782**
  - Computing grams of chosen form to hit CME target `cme_g`:
    - `form_g = round_to_0.25( cme_g * 0.879 / base_frac[form] )`
  - Unsupported / not-recommended forms:
    - **ethyl-ester**: disallow; suggest monohydrate (inferior outcomes)
    - **citrate/malate/nitrate/magnesium-chelate**: equivalence **unsupported** unless label states creatine base; prefer monohydrate
  - Always display: "≈ equivalent to X g creatine monohydrate (CME)".
- Caffeine: **3–6 mg/kg** pre (cap if sensitive; avoid late). Evidence A.
- Beta-alanine: **3.2–6.4 g/day** split. Evidence B.
- **Protein GAP logic (diet-first)**:
  - Targets (g/kg/day):
    - strength/hypertrophy **1.6–2.2** (use **1.8** for gap math)
    - endurance **1.2–1.6** (use **1.4**)
    - weight_loss **1.8–2.4** (use **2.0**)
    - general **1.4–1.6** (use **1.5**)
  - Inputs: `diet_protein_g_per_day` or `diet_protein_g_per_kg` (optional)
  - Gap calc:
    - `target = target_per_kg(goal) * weight_kg`
    - `diet = diet_g_per_day or (diet_g_per_kg * weight_kg) or 0`
    - `gap_g = max(0, target - diet)` → round to nearest **5 g**
  - Include protein supplement **only if gap ≥ 20 g/day**.
  - Show servings: `ceil(gap_g / 25)` (assume **25 g protein/serving**).
  - Notes: "Prefer food first; supplement only to close the gap." "Kidney disease → consult clinician."
- Optional adds (goal-dependent): nitrate/citrulline (B/C), betaine (C), etc., with concise "why" + citations.

### REQUIRED helper shapes (for Cursor to implement)
- `creatine_plan_by_form(weight_kg: float, form: "monohydrate"|"anhydrous"|"hcl"|None, include_loading: bool) -> dict`
  - Returns: `{supplement:"creatine", form, doses:[{value,unit,days?,split?}], timing, evidence:"A", why, notes:[...] }`
- `protein_gap_plan(goal:str, weight_kg:float, diet_g_per_day:float|None, diet_g_per_kg:float|None, serving_protein_g:int=25, include_threshold_g:int=20) -> dict|None`
  - Returns None if gap < threshold; otherwise a plan with **supplemental grams/day** and servings.

## Evidence attachment rules (stacks & summaries)
- Rank: **meta-analysis > RCT > crossover > other**; newer > older
- Dedupe by DOI
- Fields: `title, doi, pmid, url_pub, journal, year, study_type`
- Caps: ≤ **3 citations per stack item**; summaries use ≤ **8 key** and ≤ **6 recent**

## Interactions & safety
- Normalize meds to classes: {ssri, maoi, anticoag, bp, others}
- openFDA label scan → warnings (e.g., stimulant/nitrate terms) with source URLs
- Hard blocks:
  - **MAOI + stimulants** → remove caffeine/related
  - **Anticoagulants + high-dose nitrates** → reduce/flag
  - **Pregnancy/minors** (future fields) → conservative defaults
- Recompose stack after removals.

## Environment variables (set at deploy; no secrets in repo)
# Shared
FOUNDATION_ENDPOINT
FOUNDATION_KEY
FOUNDATION_API_VERSION=2024-05-01-preview
FOUNDATION_CHAT_MODEL=gpt-4o-mini
FOUNDATION_EMBED_MODEL=text-embedding-3-small
SEARCH_ENDPOINT
SEARCH_QUERY_KEY
SEARCH_ADMIN_KEY
INDEX_VERSION=v1-2025-09-25     # Current index version (update when schema changes)
CORS_ALLOW_ORIGINS=https://www.evidentfit.com

# Module A (ingest - get_papers)
SEARCH_INDEX=evidentfit-index
PM_SEARCH_QUERY=<PubMed boolean with RT context; exclude NO₂ pollution terms, clinical/disease populations>
NCBI_EMAIL=<you@example.com>
NCBI_API_KEY=<optional>
WATERMARK_KEY=meta:last_ingest
INGEST_LIMIT=15000
LOG_LEVEL=info

# Agents E & F (API)
SEARCH_INDEX=evidentfit-index
SEARCH_SUMMARIES_INDEX=evidentfit-summaries
BANKING_CACHE_DIR=.                 # banking JSON files stored in API directory
PORT=8000
BASIC_AUTH_USER, BASIC_AUTH_PASS   # optional preview gate

# Agent B (paper processor)
PAPER_PROCESSOR_USE_CLOUD=1              # Use GPT-4o-mini (default). Set to 0 for legacy Mistral-7B local GPU
FOUNDATION_ENDPOINT                      # Azure AI Foundry endpoint (required for GPT-4o-mini)
FOUNDATION_KEY                           # Azure AI Foundry API key (required for GPT-4o-mini)
FOUNDATION_CHAT_MODEL=gpt-4o-mini        # Model name (default: gpt-4o-mini)

# Agent D (summarizer job)
SEARCH_PAPERS_INDEX=evidentfit-index
SEARCH_SUMMARIES_INDEX=evidentfit-summaries
SUPPLEMENTS=creatine,caffeine,beta-alanine,citrulline,protein,hmb,nitrate,nitric-oxide,tribulus,d-aspartic-acid,deer-antler,boron,tongkat-ali,betaine,taurine,carnitine,zma,glutamine,cla,ecdysteroids,exogenous-ketones
MONTH_WINDOW=12
LOG_LEVEL=info

## Build & deploy (expected)
- Docker tags: commit SHA or semver; avoid `latest`.
- Images:
  - API → `justinmelunis/evidentfit-api:<tag>`
  - Ingest → `justinmelunis/evidentfit-ingest:<tag>`
  - Summarizer → `justinmelunis/evidentfit-summarizer:<tag>`
- SWA: Next.js static export; set `NEXT_PUBLIC_API_BASE` at build time.

## CI/CD (GitLab) basics
- Build API when `api/**/*` or `shared/**/*` change
- Build ingest when `agents/ingest/**/*` or `shared/**/*` change
- Build summarizer when `agents/summarize/**/*` or `shared/**/*` change
- Build paper_processor when `agents/paper_processor/**/*` or `shared/**/*` change
- Build banking when `agents/banking/**/*` or `shared/**/*` change
- Deploy SWA when `web/**/*` changes
- Container App/Job deploys can remain manual initially

## Health & probes
- `/healthz` for readiness & liveness
- Suggested: readiness initialDelay 15s; liveness 20s

## Coding standards
- Python: black + isort; httpx with timeouts; small retry on 429/5xx; type hints
- JS/TS: Prettier + ESLint; keep fetch logic in a small client module
- Logging: status, URL (no secrets), first ~200 chars of body on failures

## Safety, privacy, compliance
- Append "Educational only; not medical advice" to end-user outputs
- No PII beyond minimal profile fields; hash user IDs if logging
- Secrets via Azure env or Key Vault references (later with managed identity)

## Common tasks Cursor should handle (exact)
1) Implement `creatine_plan_by_form()` (CME) and `protein_gap_plan()` in `api/stack_rules.py`, and wire into Agent F builder.
2) Add `GET /summaries/{supp}` in `api/main.py` per contract.
3) Implement `POST /stack`, `GET /stack/bucket/{bucket_key}`, optional `GET /stack/buckets`.
4) Ingest (agents/ingest/get_papers/): expand supplement tags; drop NO₂ pollution false positives; filter clinical/disease populations; respect watermark (`bootstrap|monthly`).
5) Summarizer (agents/summarize/run.py): rank, compose overview/last12, read Level 1 for grade, upsert to Summaries.
6) Banking (agents/banking/run.py, level3/run.py): compute Level 1/2 from cards; compile L3 rules to config/compiled_rules.json.
6) CORS: honor `CORS_ALLOW_ORIGINS`; fix browser CORS issues.
7) Honor `INDEX_VERSION` everywhere; read from `VERSION` if present.

## Do NOT do
- Don't switch to Azure OpenAI "deployments" API; always use Foundry Project endpoints.
- Don't increase token limits to mask retrieval issues.
- Don't emit citations not present in retrieved docs.

## Code Change Protocol
Before making any code modifications:
1. **Describe the proposed change** and explain the reasoning behind it
2. **Explain the benefits** and potential trade-offs of the modification
3. **Ask for explicit approval** before implementing any changes
4. **Wait for user decision** before proceeding with code edits
5. **Ask clarifying questions** about priorities and preferences if needed

This ensures the user maintains control over what gets implemented and when, allowing for proper discussion of trade-offs and priorities.

# End of rules
